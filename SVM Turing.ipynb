{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import pickle\n",
    "\n",
    "import OU\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/Users/answer/Desktop/paper/DATA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = np.load(save_dir + \"/info.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "splits = []\n",
    "combined_df = pd.DataFrame()\n",
    "combined_labels = pd.Series()\n",
    "\n",
    "for i in range(len(info)):\n",
    "    train = info[i]['train']['df_scale'].copy()\n",
    "    train_labels = info[i]['train']['labels'].copy()\n",
    "    \n",
    "    test = info[i]['test']['df_scale'].copy()\n",
    "    test_labels = info[i]['test']['labels'].copy()\n",
    "    \n",
    "    train_len = train.shape[0]\n",
    "    test_len = test.shape[0]\n",
    "    \n",
    "    # Append rows to dataframe\n",
    "    #multi_cv_df = multi_cv_df.append(train, ignore_index=True)\n",
    "    #multi_cv_labels = multi_cv_labels.append(train_labels, ignore_index=True)\n",
    "    \n",
    "    combined_df = combined_df.append(train, ignore_index=True)\n",
    "    combined_labels = combined_labels.append(train_labels, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    # Append labels to a dataframe\n",
    "    combined_df = combined_df.append(test, ignore_index=True)\n",
    "    combined_labels = combined_labels.append(test_labels, ignore_index=True)\n",
    "    \n",
    "    # Append the indices of the folds to a list\n",
    "    splits.append((combined_df.iloc[-train_len-test_len:-test_len].index, combined_df.iloc[-test_len:].index))\n",
    "    \n",
    "    # Quality Assurance\n",
    "    assert(np.array_equal(combined_df.loc[splits[i][0]].values, train.values))\n",
    "    assert(np.array_equal(combined_labels.loc[splits[i][0]].values, train_labels.values))\n",
    "    assert(np.array_equal(combined_df.loc[splits[i][1]], test.values))\n",
    "    assert(np.array_equal(combined_labels.loc[splits[i][1]], test_labels))\n",
    "    \n",
    "splits = np.array(splits)\n",
    "\n",
    "np.save(save_dir + 'splits.npy', splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv(save_dir + 'df.csv')\n",
    "combined_labels.to_csv(save_dir + 'labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [{ 'kernel': ['rbf'],\n",
    "            'C': [0.1,1,10,100], \n",
    "            'gamma': [1, 0.1, 0.001, 0.0001], \n",
    "            'cache_size': [2000], \n",
    "            'class_weight': [{0: 0.5, 1: 0.5}, {0: 0.6, 1: 0.4}, \n",
    "                             {0: 0.7, 1: 0.3}, {0: 0.8, 1: 0.2}]\n",
    "          }, \n",
    "          { 'kernel': ['poly'], \n",
    "            'C': [0.1, 1,10,100,], \n",
    "            'gamma': [1, 0.1, 0.001, 0.0001],\n",
    "            'degree': [3, 5],\n",
    "            'cache_size': [2000],\n",
    "            'class_weight': [{0: 0.5, 1: 0.5}, \n",
    "                             {0: 0.6, 1: 0.4}, {0: 0.7, 1: 0.3}]\n",
    "          }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 448 folds for each of 160 candidates, totalling 71680 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   11.7s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   25.7s\n",
      "[Parallel(n_jobs=-1)]: Done 948 tasks      | elapsed:   45.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1848 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2948 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4248 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 5748 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 7448 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 9348 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 11448 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done 13748 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done 16248 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=-1)]: Done 18948 tasks      | elapsed: 10.1min\n",
      "[Parallel(n_jobs=-1)]: Done 21848 tasks      | elapsed: 11.6min\n",
      "[Parallel(n_jobs=-1)]: Done 24196 tasks      | elapsed: 13.7min\n",
      "[Parallel(n_jobs=-1)]: Done 26054 tasks      | elapsed: 15.4min\n",
      "[Parallel(n_jobs=-1)]: Done 28384 tasks      | elapsed: 17.2min\n",
      "[Parallel(n_jobs=-1)]: Done 32084 tasks      | elapsed: 18.9min\n",
      "[Parallel(n_jobs=-1)]: Done 35984 tasks      | elapsed: 20.8min\n",
      "[Parallel(n_jobs=-1)]: Done 39654 tasks      | elapsed: 23.0min\n",
      "[Parallel(n_jobs=-1)]: Done 43220 tasks      | elapsed: 26.7min\n",
      "[Parallel(n_jobs=-1)]: Done 47244 tasks      | elapsed: 31.3min\n",
      "[Parallel(n_jobs=-1)]: Done 50952 tasks      | elapsed: 38.2min\n",
      "[Parallel(n_jobs=-1)]: Done 55548 tasks      | elapsed: 49.4min\n",
      "[Parallel(n_jobs=-1)]: Done 59444 tasks      | elapsed: 86.6min\n",
      "[Parallel(n_jobs=-1)]: Done 63880 tasks      | elapsed: 116.4min\n",
      "[Parallel(n_jobs=-1)]: Done 68248 tasks      | elapsed: 199.8min\n",
      "[Parallel(n_jobs=-1)]: Done 71680 out of 71680 | elapsed: 307.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=[array([RangeIndex(start=0, stop=2000, step=1),\n",
       "       RangeIndex(start=2000, stop=2100, step=1)], dtype=object),\n",
       "                 array([RangeIndex(start=2100, stop=4100, step=1),\n",
       "       RangeIndex(start=4100, stop=4200, step=1)], dtype=object),\n",
       "                 array([RangeIndex(start=4200, stop=6200, step=1),\n",
       "       RangeIndex(start=6200, stop=6300, step=1)], dtype=object),\n",
       "                 array([RangeIndex(start=6300, s...\n",
       "             param_grid=[{'C': [0.1, 1, 10, 100], 'cache_size': [2000],\n",
       "                          'class_weight': [{0: 0.5, 1: 0.5}, {0: 0.6, 1: 0.4},\n",
       "                                           {0: 0.7, 1: 0.3}, {0: 0.8, 1: 0.2}],\n",
       "                          'gamma': [1, 0.1, 0.001, 0.0001], 'kernel': ['rbf']},\n",
       "                         {'C': [0.1, 1, 10, 100], 'cache_size': [2000],\n",
       "                          'class_weight': [{0: 0.5, 1: 0.5}, {0: 0.6, 1: 0.4},\n",
       "                                           {0: 0.7, 1: 0.3}],\n",
       "                          'degree': [3, 5], 'gamma': [1, 0.1, 0.001, 0.0001],\n",
       "                          'kernel': ['poly']}],\n",
       "             refit=False, scoring=['precision'], verbose=1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridcv = GridSearchCV(svm.SVC(), params, verbose=1, cv=list(splits), n_jobs=-1, \n",
    "                    scoring=['precision'], refit=False)\n",
    "\n",
    "gridcv.fit(combined_df, combined_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
